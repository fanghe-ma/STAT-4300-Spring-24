\chapter{Class 21}

\section{Covariance}
\begin{framed}
   \textbf{Definition}: For r.v. $X, Y$, the \textbf{covariance} is defined as 
    \[
       Cov(X, Y) = E \left[(X - E[X]) \cdot \left(Y - E[Y] \right) \right]
   \] 
   Equivalently,

    \[
       Cov(X, Y) = E [XY] - E[X] E[Y]
   \] 

   Note that
   \[
     Var(X) = Cov(X, X)
   \] 
\end{framed}
\textbf{Properties of covariance}
\begin{itemize}
   \item Independent r.v.s have zero covariance
      \[
        X \perp Y \implies Cov(X, Y)
      \] 
   \item Symmetry
      \[
        Cov(X, Y) = Cov(Y, X)
      \] 
   
   \item Covariance between r.v. and constant is zero
      \[
        Cov(X, c) = 0 \text{ for any constant } c
      \] 
   \item Scaling by a constant
   \[
     Cov(aX, Y) = aCov(X, Y)
   \] 
   \item Sum of two random variables
   \[
     Cov(X_1 + X_2, Y) = Cov(X_1, Y) + Cov(X_2, Y)
   \] 
   \item Covariance of sum of r.v.s
   \[
     Cov(X_1 + X_2, Y_1 + Y_2) = Cov(X_1, Y_1) + Cov(X_1, Y_2) + Cov(X_2, Y_1) + Cov(X_2, Y_2)
   \] 
\item More generally,
   \[
      Cov \left( \sum_{i = 1}^{m} X_i, \sum_{j = 1}^{n} Y_j \right) = \sum_{i = 1}^{m} \sum_{j = 1}^{n} Cov(X_i, Y_j )
   \] 
\end{itemize}

\textbf{Proofs of properties} \\

Proof of (1):
\begin{align*}
   X \perp Y &\implies E[XY] = E[X] E[Y] \\
             &\implies Cov(X, Y) = E[XY] - E[X] E[Y] = E[XY] - E[XY] = 0
\end{align*}

Proof of (2) 
\begin{align*}
   Cov(X, Y) &= E[XY] - E[X]E[Y] \\
            &= E[YX] - E[Y]E[X] \\
            &= Cov(Y, X)
\end{align*}

Proof of (3) 
\begin{align*}
   Cov(X, c) &= E[cX] - E[c]E[X] \\
             &= cE[X] - cE[X] \\
             &= 0
\end{align*}

Proof of (4)
\begin{align*}
   Cov(aX, Y) &= E[aXY] - E[aX]E[Y] \\
              &= aE[XY] - aE[X]E[Y] \\
              &= a \left( E[XY] - E[X] E[Y] \right)  \\
              &= a Cov(X, Y)
\end{align*}

Proof of (5)
\begin{align*}
   Cov(X_1 + X_2, Y) &= E[(X_1 + X_2)Y] - E[X_1 + X_2]E[Y] \\
              &= E[X_1 Y + X_2 Y]  - E[X_1 + X_2]E[Y] \\
              &= E[X_1 Y] + E[X_2 Y] - E[X_1 ] E[Y] - E[X_2] E[Y] \\
              &= E[X_1 Y] - E[X_1 ] E[Y] + E[X_2 Y]- E[X_2] E[Y] \\
              &= Cov(X_1, Y) + Cov(X_2, Y)
\end{align*}

Proof of (6) 
\begin{align*}
   &Cov(X_1 + X_2, Y_1 + Y_2)  \\
   =& Cov(X_1, Y_1 + Y_2) + Cov(X_2, Y_1 + Y_2) \quad  \text{ by Property 5} \\
   =& Cov(X_1, Y_1) + Cov(X_1, Y_2) +  Cov(X_2, Y_1) + Cov(X_2, Y_2) 
\end{align*}

\section{General case for variance of a sum of r.v.s}

\begin{framed}
   \textbf{Theorem}: For any $X, Y$
    \[
     Var(X + Y) = Var(X) + Var(Y) + 2 Cov(X, Y)
   \] 
   \[
     Var(X - Y) = Var(X) + Var(Y) - 2 Cov(X, Y)
   \] 
\end{framed}

\textbf{Proof}
\begin{align*}
   Var(X + Y) &= Cov(X+Y, X+Y) \\
              &= E\left[ (X + Y)^2\right] - E[X+Y]^2 \\
              &= E\left[ X^2 + 2XY + Y^2 \right] - \left( E[X + Y] \right)^2 \\
              &= E[X^2] + E[2XY] + E[Y^2] - E[X]^2 - 2E[X]E[Y] - E[Y]^2 \\
              &= E[X^2] - E[X]^2 + E[Y^2] - E[Y]^2 + 2 \left( E[XY] - E[X]E[Y] \right)  \\
              &= Var(X) + Var(Y) + 2Cov(X, Y)
\end{align*}

OR
\begin{align*}
&Cov(X + Y, X + Y)  \\
=& Cov(X, Y) + Cov(X, Y) +  Cov(Y, X) + Cov(Y, Y)  \\
=& Var(X) + 2Cov(X, Y) + Var(Y)
\end{align*}

\subsection{Example}
Let $X,Y \sim Unif(0, 1)$, find $Cov(X + Y, X-Y)$ \\

 \begin{align*}
    Cov( X+ Y, X - Y) &= Var(X) - Var(Y) \\
                      &= 0
\end{align*}

$X+ Y, X - Y$ not independent
\[
  X + Y = 2 \iff X = Y = 1 \iff X - Y = 0
\] 


\section{Correlation}
*not on Finals

\begin{framed}
   \textbf{Definition}: Correlation between $X, Y$ is
   \[
     Corr(X, Y) = \frac{Cov(X, Y)}{ \sqrt{Var(X) \cdot Var(Y}} = \frac{Cov(X, Y)}{sd(X) \cdot sd(Y)}
   \] 

   *Correlation is the rescaled version of $Cov(X, Y)$
\end{framed}

Properties
\begin{itemize}
   \item Location-scale transforms have no effect on correlations
      \[
        Corr(aX + b, cY + d)  = Corr(X, Y) \text{ for any } a > 0, c > 0
      \] 
   \item Correlation doesn't depend on units of measurement
   \item Correlation takes values from $-1 $ to $1$ 
      \[
        -1 \leq Corr(X, Y) \leq 1
      \] 
   \item Perfect correlation implies perfect linear relationship
      \[
        |Corr(X, Y) | = 1 \implies Y = aX + b
      \] 
\end{itemize}

\section{Multivariate Gaussian Distribution}
*not on Finals

\begin{framed}
   \textbf{Definition}: a \textbf{random vector} is an ordered list of random variables \\

   \textbf{Definition}: a random vector $(X_1 \hdots X_k)$ has the Multivariate Gaussian distribution if every linear combination of the $X_j$ s has a Gaussian distribution. \\
   \[
      (X_1 \hdots X_k) \sim MVG \text{ if } \forall \textbf{t} \in \mathbb{R}^k
   \] 
   \[
         \left( \sum_{i = 1}^{k} t_i X_i \right) \sim N(\mu, \sigma^2) \text{ for some } \mu, \sigma
   \] 
\end{framed}








